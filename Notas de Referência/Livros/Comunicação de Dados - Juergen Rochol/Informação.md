---
tags:
  - conceito
  - teoria-da-informação
  - entropia
  - rochol
  - shannon
---
Claude E. Shannon foi o primeiro a propor uma maneira quantitativa de estudar Informação, partindo do princípio de que Informação é uma função da probabilidade, uma medida associada à incerteza atrelada a cada símbolo alfabeto utilizado na comunicação.

Pois bem, se um símbolo tem 100% de chance de ocorrer em um alfabeto, não há o quê comunicar, a parte receptora sabe exatamente o que será enviado através do canal. Quanto menor a probabilidade de ocorrência de um símbolo em no alfabeto utilizado, mais informação ele carrega, porque maior é a incerteza sobre o que será 

De forma analítica, a representação da Informação deve seguir uma série de comportamentos:
- Se um símbolo tem alta probabilidade de ocorrer, a quantidade de informação que ele carrega é baixa. 
- Se um símbolo tem baixa probabilidade de ocorrer, a quantidade de informação que ele carrega é baixa.
- A quantidade de informação deve ser monotonicamente decrescente co a probabilidade de ocorrência de um símbolo.