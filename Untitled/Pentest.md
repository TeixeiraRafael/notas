## **Summary**

This page documents the process driven by the security team when working on penetration test campaigns. Outlining the steps, processes, and overall workflow of the service, it can be taken as a step-by-step guide. It is important to note that not all campaigns are driven equally, requirements may vary and adaptations may be required. Our processes are not set in stone, and updates may be required as our services evolve.   

---

## **Introduction**

A Penetration Test is only one of the many different security assessments we provide. There is no Silver Bullet when it comes to Application Security, it is important to highlight the limitations that a penetration test. Limitations come not only from the technical nature of this activity, sometimes there is simply a budget constraint that may limit how deep we can go in such an engagement.

Our most important task, the one that dictates the success or failure of a pen-test campaign is simple: **Understanding the application.** This is the main reason why we never do a pen-test without a Threat Modeling. Understanding the application, its audience, the data flows, the components and interactions, is what enables us to go deep. We should use automated tools and generalist scripts whenever possible, but we should always aim to expand on them and use the knowledge we gained over the application as leverage for context-aware testing. Reaching nuanced areas some automated tools can't reach.

A common misconception is that the goal of a pen-test is to **find** vulnerabilities. It is great when we find them, but our ultimate goal is to **search** for them. If by the end of the campaign, all we have found is some missing security headers and an outdated JavaScript library, that's perfectly fine. As long as we planned our engagement and followed it thoroughly, we did a good job.

## **Process Overview**

Our pen-testing service usually follows the following structure:  

![](https://docs.defensepoint.com/uploads/images/drawio/2024-12/UtFvNKqkXl4SAthY-drawing-14-1735329919.png)

### Process Start

The pen-testing process usually starts when the service proposal is signed by the customer. This initial process is handled by the sales team and we are usually not required to provide much input. The first deliverable of our process is a High Level pen-test plan, in this documentation we present our pen-testing process to the customer with a quick description of each step, and schedule our first Threat Modeling session with their team.

### Threat Modeling Session with the Customer

The next stage is a lightweight Threat Modeling, in this stage we try to dissect the application, learn its components, access levels, audience and data flows. We also ask the customer to show the application from a user perspective, by sharing their screen and going over the features. In some cases, multiple sessions might be required, we try not to make it longer than 2h per meeting. We should always try to record this meetings for future internal reference.  

During this session, all attendees should be taking notes and asking any questions that may arise. We don't deliver these notes to the customer, instead we use them in a follow-up step to brainstorm our engagement and come up with testing strategies we'll apply when the active testing phase comes.

To support this step, we've developed a [Threat Modeling Checklist](https://docs.google.com/spreadsheets/d/1YyBk11SiC1XZvZiz5y5SO3Exx-CSA6TuwPULPpJCutw/edit?usp=drive_link), a spreadsheet containing common questions we should ask that may indicate security flaws that we can exploit.

### Planning Meeting

After the Threat Modeling is done, all attendees from DefensePoint join a meeting where we discuss what we've learned, brainstorm which vulnerabilities we should test for and how much time will we dedicate to which test.

The main outcome of this meeting is a Pen-testing Plan, a spreadsheet listing what we have planned for internal organization purposes. An example of such planning sheet can be found [here](https://docs.google.com/spreadsheets/d/1UGigpa4w-t_QUCifkQQZU8c5Z30AAqH5qyawMO2O810/edit?usp=sharing).

In some cases, we should also update the testing plan shared with the customer at the beginning of the process. In this new version we should include our setup requirements, how many accounts will we need, what environment are we testing on and for how long will the testing environment be used by us.

### Active Testing

This is the fun part, in this step we'll be actively engaging with the application, trying out exploits and breaking things. We should follow the testing plan closely, but new ideas always come up, as long as we don't skip the planned tests, we can try out new things and explore the application at will. It is important to follow the planned engagement time, otherwise we can easily exceed the budget. Our active testing should also include automated tests, we can use the active scanner from BurpSuite, or any supporting tools that can help us catch the "low-hanging fruit". Typically missing security headers and outdated dependencies are found it this step, it is important to manually validate those findings, are False Positives can occur in automated scans.

Some engagements include a **Replayable Pen-test** delivery. This means that the testing team will develop OWASP Zap scripts that can automatically test all the planned exploits. In case we find vulnerabilities not included in the initial plan, we should create scripts to identify them as well. A full reference guide on how to develop these can be found in [this documentation](https://docs.defensepoint.com/books/security-tools/chapter/dast-engine "DAST Engine").

### Report Development

By the end of our testing, we should build a Pen-test report. This should contain a description of our tests and the results we've found, followed by a detailed list of findings. For each finding we should document an Attack Scenario, highlighting the causes and impacts of a successful exploit to the vulnerability. We should always include fix suggestions and evidences that confirm the finding. Assigning a Risk-level to a finding can be a bit subjective, as it usually relies heavily on the context, this should always be discussed with the team to reach a consensus.  

It is important to note that our findings must be **actionable**. We need to give the customer something they can work with, findings must be reproducible and have clear explanations of their impact..

Finally, we should always use our report template and make sure all contact information is up-to-date. A pen-test report example can be found [here](https://docs.google.com/document/d/1WaRcJiARu_l6DMzmXwF4HmHZLpKXLlhfdjlK5f1rljo/edit?usp=sharing).

### Results Presentation

Sometimes customers may ask for a quick presentation of our results, in this case, we'll build a quick slideshow with an overview of our findings and perform a live demo of how we were able to identify the issue.

### Re-testing and Validating

 

[](https://docs.defensepoint.com/books/security-processes-documentation/page/penetration-tests/edit?content-id=bkmrk-after-our-report-is-&content-text=After%20our%20report%20is%20delivered%2C%20we%20let%20the%20customer "Jump to section in editor")

After our report is delivered, we let the customer take their time to fix the issues. Once they come back with patches, our task is to re-test our findings and confirm that the fixes really resolved the issue. Iterating with the development team if needed, until the vulnerability is fully resolved. At this stage, we can also write-up an updated report and an Executive Summary, highlighting that the findings were addressed and no longer affect the application.